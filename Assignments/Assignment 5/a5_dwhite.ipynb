{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1. Traversing a Web Graph (8 points)\n",
    "\n",
    "You begin by building a web crawler function, webCrawler, that traverses a web graph consisting of a self-contained set of linked web pages.\n",
    "\n",
    "First, you can use the `urllib` package to retrieve web pages as follows:\n",
    "\n",
    "```\n",
    "\n",
    "import urllib.request\n",
    "webUrl  = urllib.request.urlopen('https://ischool.berkeley.edu/')\n",
    "data = webUrl.read()\n",
    "\n",
    "```\n",
    "\n",
    "Starting with the following URL:\n",
    "\n",
    "[https://people.ischool.berkeley.edu/~chuang/i206/b5/index.html](https://people.ischool.berkeley.edu/~chuang/i206/b5/index.html)\n",
    "\n",
    "Your crawler should identify and follow the links on the page, as well as the links found on the other pages reachable from this source page, using the breadth-first search (BFS) technique. \n",
    "\n",
    "You can use regex (which you have now mastered) or the [Beautiful Soup](https://www.crummy.com/software/BeautifulSoup/bs4/doc/). library (e.g., its `findAll()` and `get()` methods) to find the links on each page. To simplify your task, all the links in this set of web pages use relative links, e.g., \n",
    "\n",
    "\\<a href=”somepage.html”>This is a link to some random page</a>\n",
    "\n",
    "which should be resolved to:\n",
    "\n",
    "[https://people.ischool.berkeley.edu/~chuang/i206/b5/somepage.html](https://people.ischool.berkeley.edu/~chuang/i206/b5/somepage.html)\n",
    "\n",
    "Note that pages may link to one another via loops, e.g., A links to B, B links to C, and C links back to A. Your crawler has to avoid loops by keeping track of which pages have already been visited (or not), so that you don't visit the same pages again. Use Python's `deque` data structure to implement a queue for this purpose.\n",
    "\n",
    "Note: please do not try to run your code on the open Web unless you have properly implemented the following: (i) checking and conforming to a site’s robots.txt file, (ii) rate-limiting your crawler, (iii) properly resolving fully specified and relative links. Otherwise you may get a nasty call from someone.\n",
    "\n",
    "Upon completion of the crawl, your crawler function should return the following:\n",
    "\n",
    "A list of the pages found (following the exact order in which they were visited, starting with `index.html`)\n",
    "Total number of pages crawled (including `index.html`)\n",
    "Total number of links found\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
