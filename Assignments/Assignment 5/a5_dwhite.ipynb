{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1. Traversing a Web Graph (8 points)\n",
    "\n",
    "You begin by building a web crawler function, webCrawler, that traverses a web graph consisting of a self-contained set of linked web pages.\n",
    "\n",
    "First, you can use the `urllib` package to retrieve web pages as follows:\n",
    "\n",
    "```\n",
    "import urllib.request\n",
    "webUrl  = urllib.request.urlopen('https://ischool.berkeley.edu/')\n",
    "data = webUrl.read()\n",
    "```\n",
    "\n",
    "Starting with the following URL:\n",
    "\n",
    "[https://people.ischool.berkeley.edu/~chuang/i206/b5/index.html](https://people.ischool.berkeley.edu/~chuang/i206/b5/index.html)\n",
    "\n",
    "Your crawler should identify and follow the links on the page, as well as the links found on the other pages reachable from this source page, using the breadth-first search (BFS) technique. \n",
    "\n",
    "You can use regex (which you have now mastered) or the [Beautiful Soup](https://www.crummy.com/software/BeautifulSoup/bs4/doc/). library (e.g., its `findAll()` and `get()` methods) to find the links on each page. To simplify your task, all the links in this set of web pages use relative links, e.g., \n",
    "\n",
    "\\<a href=”somepage.html”>This is a link to some random page\\</a>\n",
    "\n",
    "which should be resolved to:\n",
    "\n",
    "[https://people.ischool.berkeley.edu/~chuang/i206/b5/somepage.html](https://people.ischool.berkeley.edu/~chuang/i206/b5/somepage.html)\n",
    "\n",
    "Note that pages may link to one another via loops, e.g., A links to B, B links to C, and C links back to A. Your crawler has to avoid loops by keeping track of which pages have already been visited (or not), so that you don't visit the same pages again. Use Python's `deque` data structure to implement a queue for this purpose.\n",
    "\n",
    "Note: please do not try to run your code on the open Web unless you have properly implemented the following: (i) checking and conforming to a site’s robots.txt file, (ii) rate-limiting your crawler, (iii) properly resolving fully specified and relative links. Otherwise you may get a nasty call from someone.\n",
    "\n",
    "Upon completion of the crawl, your crawler function should return the following:\n",
    "\n",
    "* A list of the pages found (following the exact order in which they were visited, starting with `index.html`)\n",
    "* Total number of pages crawled (including `index.html`)\n",
    "* Total number of links found\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Drake's notes #####\n",
    "# make a list of all of them, even with duplicates, and then just iterate over them\n",
    "# don't visit the duplicates? \n",
    "\n",
    "# nvm - this would just go infinitely because we won't know when we run out of content\n",
    "\n",
    "# so we would need to recur for every web page, and stop when we don't find any new URLs \n",
    "\n",
    "# something like while deque != 0?\n",
    "\n",
    "# how do we make it recursive? \n",
    "    # we don't have to\n",
    "\n",
    "# notes for future drake\n",
    "# https://www.geeksforgeeks.org/deque-in-python/\n",
    "# https://www.youtube.com/watch?v=oDqjPvD54Ss\n",
    "\n",
    "# so we need to us de.count() to check to make sure it isn't already in the queue. \n",
    "# if it isn't then we don't count it.\n",
    "\n",
    "# we also need to count how many pages visited, how many links found, and the list of pages visited\n",
    "# in a specific order\n",
    "\n",
    "\n",
    "# get the url\n",
    "# collect all links in the url\n",
    "# visit each of those links\n",
    "    # get all the links in those websites \n",
    "    # if there are links on the website that aren't in the deque, run it on them too \n",
    "    # if link not in dequeu\n",
    "        # append it to the end \n",
    "    \n",
    "# while deque != 0 \n",
    "    # urllib(deque[1])\n",
    "    # find links\n",
    "        # for every link, check if its in the deque \n",
    "        # if it isn't, append it to deque\n",
    "    # if it is\n",
    "        # dequeue.append(link)\n",
    "    # deque.popleft() \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Web Pages: ['index.html', 'information.html', 'Berkeley.html', 'ISchool.html', 'MIMS.html', 'CityOfBerkeley.html', 'UCBerkeley.html', 'BerkeleyCollege.html', 'SouthHall.html', 'Campanile.html'] \n",
      "\n",
      "We found 10 unique pages!\n",
      "\n",
      "In total, there were 23 links!\n"
     ]
    }
   ],
   "source": [
    "#import\n",
    "import urllib.request\n",
    "import re\n",
    "from collections import deque\n",
    "\n",
    "\n",
    "# define webCrawler\n",
    "def webCrawler(link):\n",
    "\n",
    "    # start queue\n",
    "    queue = deque([])\n",
    "\n",
    "    # add the url suffix to it\n",
    "    queue.append(link)\n",
    "\n",
    "    # keep track of all web pages visited\n",
    "    web_pages = []\n",
    "\n",
    "    # unique sites visited, including index.html\n",
    "    sites_visited = [link]\n",
    "\n",
    "    # number of links found, including the one we started with\n",
    "    num_links = 1\n",
    "\n",
    "    # start the loop, keep looping until deque is empty\n",
    "    while len(queue) > 0:\n",
    "        \n",
    "        #TRACING\n",
    "        #print(queue)\n",
    "\n",
    "        # access the first entry\n",
    "        link = \"https://people.ischool.berkeley.edu/~chuang/i206/b5/\" + queue[0]\n",
    "        webUrl  = urllib.request.urlopen(link)\n",
    "        data = str(webUrl.read())\n",
    "\n",
    "        # FOR TESTING LOCALLY\n",
    "        ##file = open(queue[0], \"r\")\n",
    "        ##file_contents = file.read()\n",
    "\n",
    "        # also append to visited pages\n",
    "        web_pages.append(queue[0])\n",
    "\n",
    "        #TRACING\n",
    "        #print(\"All web pages visited:\", web_pages)\n",
    "\n",
    "        # find all the links in the first entry\n",
    "        pattern = re.compile(r'href=[\\'\"]?([^\\'\" >]+)')\n",
    "        results = pattern.findall(data)\n",
    "\n",
    "        # FOR TESTING LOCALLY\n",
    "        ##results = pattern.findall(file_contents)\n",
    "\n",
    "        # TRACING\n",
    "        #print(\"Pattern results:\", results)\n",
    "\n",
    "        # loop over all the links in the webpage\n",
    "        for each in results:\n",
    "            # iterate our counter representing the number of links we've found\n",
    "            num_links += 1\n",
    "            # if it's not in the queue, add it to the end of the queue\n",
    "            if each not in sites_visited:\n",
    "                queue.append(each)\n",
    "                # also append it to our list of unique sites visited\n",
    "                sites_visited.append(each)\n",
    "        \n",
    "        # remove the first item \n",
    "        queue.popleft()\n",
    "\n",
    "        # FOR TESTING LOCALLY\n",
    "        #file.close()\n",
    "\n",
    "    return web_pages, len(web_pages), num_links\n",
    "\n",
    "## MAIN\n",
    "\n",
    "web_pages, page_amt, link_amt = webCrawler(\"index.html\")\n",
    "\n",
    "print(\"Web Pages:\", web_pages, \"\\n\")\n",
    "\n",
    "print(\"We found\", page_amt, \"unique pages!\\n\")\n",
    "\n",
    "print(\"In total, there were\", link_amt, \"links!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2. Indexing Web Pages (6 points)\n",
    "\n",
    "Extend your web crawler from Part 1 so that as it encounters web pages, it also builds an inverted index (using the dictionary data structure) based on the words found on each web page. Call this function `webCrawlIndexer`.\n",
    "\n",
    "Each time you retrieve a new web page, you will need to extract the words from the page. You may re-use your code from Assignment 3 (sentiment analysis), or you can also use the `get_text()` method from BeautifulSoup for this purpose.\n",
    "\n",
    "When your indexer encounters a new word, it should add a new entry to the inverted index, with the word as the key, and the page name (e.g., `somepage.html`) as the value. When it encounters a word already in the index, it should update the entry to append the new page name as the value. However, if a word appears multiple times in a web page, you should not append the same web page name multiple times. For example: \n",
    "\n",
    "Correct: inv_index = {‘word1’:[’page1.html,page2.html’]}\n",
    "Incorrect: inv_index = {‘word1’:[’page1.html,page2.html,page2.html’]}\n",
    "\n",
    "Upon completion, your webCrawlIndexer function should return:\n",
    "\n",
    "* The number of entries in the inverted index\n",
    "* The inverted index dictionary data structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our word count is: 461 \n",
      "\n",
      "For the record - I did some cleanup, which the assignment did not say I needed to do. It just said to use the beautiful soup module above. If you expected us to remove every single weird entry or duplicate word with some weird characters on the end that get_text seems to find, the assignment description should have said so explicitly.\n",
      "\n",
      "206 = ['index.html']\n",
      "crawler = ['index.html']\n",
      "home = ['index.html', 'information.html', 'Berkeley.html', 'ISchool.html', 'MIMS.html', 'CityOfBerkeley.html', 'UCBerkeley.html', 'BerkeleyCollege.html', 'SouthHall.html', 'Campanile.html']\n",
      "page = ['index.html', 'information.html']\n",
      "the = ['index.html', 'information.html', 'ISchool.html', 'MIMS.html', 'CityOfBerkeley.html', 'UCBerkeley.html', 'BerkeleyCollege.html', 'SouthHall.html', 'Campanile.html']\n",
      "where = ['index.html']\n",
      "any = ['index.html', 'UCBerkeley.html']\n",
      "information = ['index.html', 'information.html', 'ISchool.html', 'MIMS.html']\n",
      "can = ['index.html', 'Berkeley.html']\n",
      "be = ['index.html', 'Berkeley.html']\n",
      "found = ['index.html']\n",
      "is = ['index.html', 'ISchool.html', 'MIMS.html', 'CityOfBerkeley.html', 'UCBerkeley.html', 'BerkeleyCollege.html', 'SouthHall.html', 'Campanile.html']\n",
      "homepage = ['index.html']\n",
      "your = ['index.html']\n",
      "will = ['index.html']\n",
      "start = ['index.html']\n",
      "follow = ['index.html']\n",
      "links = ['index.html']\n",
      "to = ['index.html', 'information.html', 'Berkeley.html', 'ISchool.html', 'MIMS.html', 'CityOfBerkeley.html', 'UCBerkeley.html', 'SouthHall.html', 'Campanile.html']\n",
      "discover = ['index.html']\n",
      "whats = ['index.html']\n",
      "on = ['index.html', 'CityOfBerkeley.html', 'UCBerkeley.html', 'SouthHall.html', 'Campanile.html']\n",
      "site = ['index.html', 'CityOfBerkeley.html']\n",
      "there = ['information.html']\n",
      "are = ['information.html', 'MIMS.html', 'UCBerkeley.html', 'Campanile.html']\n",
      "many = ['information.html']\n",
      "great = ['information.html']\n",
      "things = ['information.html', 'Berkeley.html']\n",
      "learn = ['information.html']\n",
      "about = ['information.html', 'ISchool.html', 'SouthHall.html', 'Campanile.html']\n",
      "here = ['information.html']\n",
      "berkeley = ['information.html', 'Berkeley.html', 'ISchool.html', 'CityOfBerkeley.html', 'UCBerkeley.html', 'BerkeleyCollege.html']\n",
      "uc = ['information.html', 'Berkeley.html', 'ISchool.html', 'UCBerkeley.html']\n",
      "school = ['information.html', 'ISchool.html', 'MIMS.html', 'BerkeleyCollege.html']\n",
      "of = ['information.html', 'ISchool.html', 'MIMS.html', 'CityOfBerkeley.html', 'UCBerkeley.html', 'BerkeleyCollege.html', 'SouthHall.html', 'Campanile.html']\n",
      "masters = ['information.html', 'MIMS.html']\n",
      "management = ['information.html', 'ISchool.html', 'MIMS.html', 'BerkeleyCollege.html']\n",
      "and = ['information.html', 'ISchool.html', 'MIMS.html', 'CityOfBerkeley.html', 'UCBerkeley.html', 'BerkeleyCollege.html', 'SouthHall.html', 'Campanile.html']\n",
      "systems = ['information.html', 'ISchool.html', 'MIMS.html']\n",
      "degree = ['information.html', 'ISchool.html', 'MIMS.html', 'UCBerkeley.html', 'BerkeleyCollege.html']\n",
      "which = ['Berkeley.html', 'CityOfBerkeley.html', 'UCBerkeley.html']\n",
      "refers = ['Berkeley.html']\n",
      "a = ['Berkeley.html', 'ISchool.html', 'MIMS.html', 'CityOfBerkeley.html', 'UCBerkeley.html', 'BerkeleyCollege.html', 'Campanile.html']\n",
      "few = ['Berkeley.html']\n",
      "different = ['Berkeley.html']\n",
      "you = ['Berkeley.html']\n",
      "more = ['Berkeley.html', 'UCBerkeley.html', 'Campanile.html']\n",
      "specific = ['Berkeley.html']\n",
      "california = ['Berkeley.html', 'ISchool.html', 'CityOfBerkeley.html', 'UCBerkeley.html', 'SouthHall.html']\n",
      "college = ['Berkeley.html', 'UCBerkeley.html', 'BerkeleyCollege.html', 'Campanile.html']\n",
      "or = ['ISchool.html', 'UCBerkeley.html']\n",
      "i = ['ISchool.html', 'MIMS.html']\n",
      "graduate = ['ISchool.html', 'CityOfBerkeley.html', 'UCBerkeley.html']\n",
      "offering = ['ISchool.html']\n",
      "mims = ['ISchool.html', 'MIMS.html']\n",
      "in = ['ISchool.html', 'MIMS.html', 'CityOfBerkeley.html', 'UCBerkeley.html', 'BerkeleyCollege.html', 'SouthHall.html', 'Campanile.html']\n",
      "addition = ['ISchool.html']\n",
      "mids = ['ISchool.html']\n",
      "mics = ['ISchool.html']\n",
      "degrees = ['ISchool.html', 'BerkeleyCollege.html']\n",
      "researchoriented = ['ISchool.html']\n",
      "phd = ['ISchool.html']\n",
      "at = ['ISchool.html', 'MIMS.html', 'CityOfBerkeley.html']\n",
      "university = ['ISchool.html', 'CityOfBerkeley.html', 'UCBerkeley.html', 'SouthHall.html']\n",
      "created = ['ISchool.html']\n",
      "1994 = ['ISchool.html']\n",
      "berkeleys = ['ISchool.html', 'UCBerkeley.html']\n",
      "newest = ['ISchool.html']\n",
      "it = ['ISchool.html', 'CityOfBerkeley.html', 'UCBerkeley.html', 'SouthHall.html', 'Campanile.html']\n",
      "was = ['ISchool.html', 'CityOfBerkeley.html', 'SouthHall.html', 'Campanile.html']\n",
      "previously = ['ISchool.html']\n",
      "known = ['ISchool.html', 'SouthHall.html', 'Campanile.html']\n",
      "as = ['ISchool.html', 'MIMS.html', 'CityOfBerkeley.html', 'UCBerkeley.html', 'SouthHall.html', 'Campanile.html']\n",
      "sims = ['ISchool.html']\n",
      "until = ['ISchool.html']\n",
      "2006 = ['ISchool.html']\n",
      "its = ['ISchool.html', 'UCBerkeley.html', 'Campanile.html']\n",
      "roots = ['ISchool.html']\n",
      "trace = ['ISchool.html']\n",
      "back = ['ISchool.html']\n",
      "librarianship = ['ISchool.html']\n",
      "founded = ['ISchool.html', 'UCBerkeley.html', 'BerkeleyCollege.html']\n",
      "1920s = ['ISchool.html']\n",
      "program = ['ISchool.html', 'MIMS.html']\n",
      "located = ['ISchool.html', 'UCBerkeley.html', 'SouthHall.html']\n",
      "south = ['ISchool.html', 'CityOfBerkeley.html', 'SouthHall.html']\n",
      "hall = ['ISchool.html', 'SouthHall.html']\n",
      "near = ['ISchool.html', 'SouthHall.html']\n",
      "sather = ['ISchool.html', 'SouthHall.html', 'Campanile.html']\n",
      "tower = ['ISchool.html', 'SouthHall.html', 'Campanile.html']\n",
      "center = ['ISchool.html']\n",
      "campus = ['ISchool.html', 'UCBerkeley.html', 'SouthHall.html', 'Campanile.html']\n",
      "master = ['MIMS.html', 'Campanile.html']\n",
      "48 = ['MIMS.html']\n",
      "unit = ['MIMS.html']\n",
      "twoyear = ['MIMS.html']\n",
      "designed = ['MIMS.html', 'Campanile.html']\n",
      "train = ['MIMS.html']\n",
      "students = ['MIMS.html', 'UCBerkeley.html']\n",
      "for = ['MIMS.html', 'UCBerkeley.html', 'SouthHall.html']\n",
      "careers = ['MIMS.html']\n",
      "professionals = ['MIMS.html']\n",
      "who = ['MIMS.html', 'SouthHall.html']\n",
      "complete = ['MIMS.html']\n",
      "awarded = ['MIMS.html']\n",
      "during = ['MIMS.html']\n",
      "first = ['MIMS.html', 'Campanile.html']\n",
      "year = ['MIMS.html']\n",
      "take = ['MIMS.html']\n",
      "range = ['MIMS.html', 'UCBerkeley.html']\n",
      "required = ['MIMS.html']\n",
      "elective = ['MIMS.html']\n",
      "courses = ['MIMS.html', 'BerkeleyCollege.html']\n",
      "second = ['MIMS.html']\n",
      "may = ['MIMS.html']\n",
      "choose = ['MIMS.html']\n",
      "from = ['MIMS.html']\n",
      "both = ['MIMS.html', 'UCBerkeley.html']\n",
      "other = ['MIMS.html', 'UCBerkeley.html']\n",
      "departments = ['MIMS.html']\n",
      "final = ['MIMS.html']\n",
      "requirement = ['MIMS.html']\n",
      "completion = ['MIMS.html']\n",
      "capstone = ['MIMS.html']\n",
      "project = ['MIMS.html']\n",
      "city = ['CityOfBerkeley.html']\n",
      "east = ['CityOfBerkeley.html']\n",
      "shore = ['CityOfBerkeley.html']\n",
      "san = ['CityOfBerkeley.html', 'UCBerkeley.html', 'Campanile.html']\n",
      "francisco = ['CityOfBerkeley.html', 'UCBerkeley.html']\n",
      "bay = ['CityOfBerkeley.html', 'UCBerkeley.html']\n",
      "northern = ['CityOfBerkeley.html']\n",
      "californiaxe2x80x94in = ['CityOfBerkeley.html']\n",
      "alameda = ['CityOfBerkeley.html']\n",
      "county = ['CityOfBerkeley.html']\n",
      "bordered = ['CityOfBerkeley.html']\n",
      "by = ['CityOfBerkeley.html', 'SouthHall.html', 'Campanile.html']\n",
      "cities = ['CityOfBerkeley.html']\n",
      "oakland = ['CityOfBerkeley.html', 'UCBerkeley.html']\n",
      "emeryville = ['CityOfBerkeley.html']\n",
      "north = ['CityOfBerkeley.html']\n",
      "albany = ['CityOfBerkeley.html']\n",
      "unincorporated = ['CityOfBerkeley.html']\n",
      "community = ['CityOfBerkeley.html']\n",
      "kensington = ['CityOfBerkeley.html']\n",
      "eastern = ['CityOfBerkeley.html', 'UCBerkeley.html']\n",
      "limit = ['CityOfBerkeley.html']\n",
      "coincides = ['CityOfBerkeley.html']\n",
      "with = ['CityOfBerkeley.html', 'UCBerkeley.html', 'Campanile.html']\n",
      "contra = ['CityOfBerkeley.html']\n",
      "costa = ['CityOfBerkeley.html']\n",
      "border = ['CityOfBerkeley.html']\n",
      "generally = ['CityOfBerkeley.html']\n",
      "follows = ['CityOfBerkeley.html']\n",
      "ridge = ['CityOfBerkeley.html']\n",
      "line = ['CityOfBerkeley.html']\n",
      "hills = ['CityOfBerkeley.html']\n",
      "population = ['CityOfBerkeley.html']\n",
      "112580 = ['CityOfBerkeley.html']\n",
      "2010 = ['CityOfBerkeley.html']\n",
      "census = ['CityOfBerkeley.html']\n",
      "named = ['CityOfBerkeley.html']\n",
      "after = ['CityOfBerkeley.html']\n",
      "bishop = ['CityOfBerkeley.html']\n",
      "george = ['CityOfBerkeley.html']\n",
      "oldest = ['CityOfBerkeley.html', 'UCBerkeley.html', 'SouthHall.html']\n",
      "system = ['CityOfBerkeley.html', 'UCBerkeley.html', 'SouthHall.html']\n",
      "lawrence = ['CityOfBerkeley.html', 'UCBerkeley.html']\n",
      "national = ['CityOfBerkeley.html', 'UCBerkeley.html']\n",
      "laboratory = ['CityOfBerkeley.html', 'UCBerkeley.html']\n",
      "also = ['CityOfBerkeley.html', 'UCBerkeley.html', 'SouthHall.html']\n",
      "theological = ['CityOfBerkeley.html']\n",
      "union = ['CityOfBerkeley.html']\n",
      "noted = ['CityOfBerkeley.html']\n",
      "one = ['CityOfBerkeley.html', 'UCBerkeley.html']\n",
      "most = ['CityOfBerkeley.html']\n",
      "politically = ['CityOfBerkeley.html']\n",
      "liberal = ['CityOfBerkeley.html', 'BerkeleyCollege.html']\n",
      "nation = ['CityOfBerkeley.html', 'UCBerkeley.html']\n",
      "referred = ['UCBerkeley.html']\n",
      "simply = ['UCBerkeley.html']\n",
      "cal = ['UCBerkeley.html']\n",
      "public = ['UCBerkeley.html', 'Campanile.html']\n",
      "research = ['UCBerkeley.html']\n",
      "united = ['UCBerkeley.html']\n",
      "states = ['UCBerkeley.html']\n",
      "occupies = ['UCBerkeley.html']\n",
      "1232 = ['UCBerkeley.html']\n",
      "acres = ['UCBerkeley.html']\n",
      "499 = ['UCBerkeley.html']\n",
      "ha = ['UCBerkeley.html']\n",
      "side = ['UCBerkeley.html']\n",
      "central = ['UCBerkeley.html']\n",
      "resting = ['UCBerkeley.html']\n",
      "178 = ['UCBerkeley.html']\n",
      "72 = ['UCBerkeley.html']\n",
      "flagship = ['UCBerkeley.html']\n",
      "institution = ['UCBerkeley.html', 'BerkeleyCollege.html']\n",
      "10 = ['UCBerkeley.html']\n",
      "only = ['UCBerkeley.html']\n",
      "two = ['UCBerkeley.html']\n",
      "campuses = ['UCBerkeley.html']\n",
      "operating = ['UCBerkeley.html']\n",
      "semester = ['UCBerkeley.html']\n",
      "calendar = ['UCBerkeley.html']\n",
      "being = ['UCBerkeley.html']\n",
      "merced = ['UCBerkeley.html']\n",
      "established = ['UCBerkeley.html']\n",
      "1868 = ['UCBerkeley.html']\n",
      "result = ['UCBerkeley.html']\n",
      "merger = ['UCBerkeley.html']\n",
      "private = ['UCBerkeley.html']\n",
      "agricultural = ['UCBerkeley.html']\n",
      "mining = ['UCBerkeley.html']\n",
      "mechanical = ['UCBerkeley.html']\n",
      "arts = ['UCBerkeley.html', 'BerkeleyCollege.html']\n",
      "offers = ['UCBerkeley.html', 'BerkeleyCollege.html']\n",
      "approximately = ['UCBerkeley.html']\n",
      "350 = ['UCBerkeley.html']\n",
      "undergraduate = ['UCBerkeley.html']\n",
      "programs = ['UCBerkeley.html', 'BerkeleyCollege.html']\n",
      "wide = ['UCBerkeley.html']\n",
      "disciplines = ['UCBerkeley.html']\n",
      "has = ['UCBerkeley.html', 'Campanile.html']\n",
      "been = ['UCBerkeley.html', 'Campanile.html']\n",
      "charged = ['UCBerkeley.html']\n",
      "providing = ['UCBerkeley.html']\n",
      "classical = ['UCBerkeley.html']\n",
      "practical = ['UCBerkeley.html']\n",
      "education = ['UCBerkeley.html', 'BerkeleyCollege.html']\n",
      "people = ['UCBerkeley.html']\n",
      "comanages = ['UCBerkeley.html']\n",
      "three = ['UCBerkeley.html']\n",
      "department = ['UCBerkeley.html']\n",
      "energy = ['UCBerkeley.html']\n",
      "laboratories = ['UCBerkeley.html']\n",
      "including = ['UCBerkeley.html', 'BerkeleyCollege.html']\n",
      "los = ['UCBerkeley.html']\n",
      "alamos = ['UCBerkeley.html']\n",
      "livermore = ['UCBerkeley.html']\n",
      "us = ['UCBerkeley.html']\n",
      "among = ['UCBerkeley.html']\n",
      "alumni = ['UCBerkeley.html']\n",
      "faculty = ['UCBerkeley.html']\n",
      "researchers = ['UCBerkeley.html']\n",
      "nobel = ['UCBerkeley.html']\n",
      "laureates = ['UCBerkeley.html']\n",
      "110 = ['UCBerkeley.html']\n",
      "turing = ['UCBerkeley.html']\n",
      "award = ['UCBerkeley.html']\n",
      "winners = ['UCBerkeley.html']\n",
      "25 = ['UCBerkeley.html']\n",
      "fields = ['UCBerkeley.html']\n",
      "medalists = ['UCBerkeley.html']\n",
      "14 = ['UCBerkeley.html']\n",
      "wolf = ['UCBerkeley.html']\n",
      "prize = ['UCBerkeley.html']\n",
      "30 = ['UCBerkeley.html']\n",
      "than = ['UCBerkeley.html']\n",
      "affiliated = ['UCBerkeley.html']\n",
      "34 = ['UCBerkeley.html']\n",
      "pulitzer = ['UCBerkeley.html']\n",
      "prizes = ['UCBerkeley.html']\n",
      "19 = ['UCBerkeley.html']\n",
      "academy = ['UCBerkeley.html']\n",
      "awards = ['UCBerkeley.html']\n",
      "macarthur = ['UCBerkeley.html']\n",
      "genius = ['UCBerkeley.html']\n",
      "grants = ['UCBerkeley.html']\n",
      "108 = ['UCBerkeley.html']\n",
      "medals = ['UCBerkeley.html']\n",
      "science = ['UCBerkeley.html']\n",
      "68 = ['UCBerkeley.html']\n",
      "date = ['UCBerkeley.html']\n",
      "associated = ['UCBerkeley.html']\n",
      "6 = ['UCBerkeley.html']\n",
      "chemical = ['UCBerkeley.html']\n",
      "elements = ['UCBerkeley.html']\n",
      "periodic = ['UCBerkeley.html']\n",
      "table = ['UCBerkeley.html']\n",
      "californium = ['UCBerkeley.html']\n",
      "seaborgium = ['UCBerkeley.html']\n",
      "berkelium = ['UCBerkeley.html']\n",
      "einsteinium = ['UCBerkeley.html']\n",
      "fermium = ['UCBerkeley.html']\n",
      "lawrencium = ['UCBerkeley.html']\n",
      "lab = ['UCBerkeley.html']\n",
      "discovered = ['UCBerkeley.html']\n",
      "16 = ['UCBerkeley.html']\n",
      "total = ['UCBerkeley.html']\n",
      "xe2x80x93 = ['UCBerkeley.html']\n",
      "worldt = ['UCBerkeley.html']\n",
      "produced = ['UCBerkeley.html']\n",
      "seven = ['UCBerkeley.html', 'Campanile.html']\n",
      "heads = ['UCBerkeley.html']\n",
      "state = ['UCBerkeley.html']\n",
      "government = ['UCBerkeley.html']\n",
      "six = ['UCBerkeley.html']\n",
      "chief = ['UCBerkeley.html']\n",
      "justices = ['UCBerkeley.html']\n",
      "justice = ['UCBerkeley.html', 'BerkeleyCollege.html']\n",
      "earl = ['UCBerkeley.html']\n",
      "warren = ['UCBerkeley.html']\n",
      "22 = ['UCBerkeley.html']\n",
      "cabinetlevel = ['UCBerkeley.html']\n",
      "officials = ['UCBerkeley.html']\n",
      "11 = ['UCBerkeley.html']\n",
      "governors = ['UCBerkeley.html']\n",
      "living = ['UCBerkeley.html']\n",
      "billionaires = ['UCBerkeley.html']\n",
      "leading = ['UCBerkeley.html']\n",
      "producer = ['UCBerkeley.html']\n",
      "fulbright = ['UCBerkeley.html']\n",
      "scholars = ['UCBerkeley.html']\n",
      "fellows = ['UCBerkeley.html']\n",
      "marshall = ['UCBerkeley.html']\n",
      "widely = ['UCBerkeley.html']\n",
      "recognized = ['UCBerkeley.html']\n",
      "their = ['UCBerkeley.html']\n",
      "entrepreneurship = ['UCBerkeley.html', 'BerkeleyCollege.html']\n",
      "have = ['UCBerkeley.html', 'SouthHall.html']\n",
      "numerous = ['UCBerkeley.html']\n",
      "notable = ['UCBerkeley.html']\n",
      "companies = ['UCBerkeley.html']\n",
      "apple = ['UCBerkeley.html']\n",
      "tesla = ['UCBerkeley.html']\n",
      "intel = ['UCBerkeley.html']\n",
      "ebay = ['UCBerkeley.html']\n",
      "softbank = ['UCBerkeley.html']\n",
      "aig = ['UCBerkeley.html']\n",
      "morgan = ['UCBerkeley.html']\n",
      "stanleyt = ['UCBerkeley.html']\n",
      "athletic = ['UCBerkeley.html']\n",
      "teams = ['UCBerkeley.html']\n",
      "compete = ['UCBerkeley.html']\n",
      "golden = ['UCBerkeley.html']\n",
      "bears = ['UCBerkeley.html']\n",
      "primarily = ['UCBerkeley.html']\n",
      "pac12 = ['UCBerkeley.html']\n",
      "conference = ['UCBerkeley.html']\n",
      "won = ['UCBerkeley.html']\n",
      "107 = ['UCBerkeley.html']\n",
      "championships = ['UCBerkeley.html']\n",
      "223 = ['UCBerkeley.html']\n",
      "olympic = ['UCBerkeley.html']\n",
      "121 = ['UCBerkeley.html']\n",
      "gold = ['UCBerkeley.html']\n",
      "higher = ['BerkeleyCollege.html']\n",
      "1931 = ['BerkeleyCollege.html']\n",
      "specializing = ['BerkeleyCollege.html']\n",
      "business = ['BerkeleyCollege.html']\n",
      "professional = ['BerkeleyCollege.html']\n",
      "studies = ['BerkeleyCollege.html']\n",
      "associates = ['BerkeleyCollege.html']\n",
      "bachelors = ['BerkeleyCollege.html']\n",
      "provides = ['BerkeleyCollege.html']\n",
      "academic = ['BerkeleyCollege.html']\n",
      "administration = ['BerkeleyCollege.html']\n",
      "general = ['BerkeleyCollege.html']\n",
      "accounting = ['BerkeleyCollege.html']\n",
      "financial = ['BerkeleyCollege.html']\n",
      "services = ['BerkeleyCollege.html']\n",
      "marketing = ['BerkeleyCollege.html']\n",
      "communications = ['BerkeleyCollege.html']\n",
      "concentrations = ['BerkeleyCollege.html']\n",
      "environmental = ['BerkeleyCollege.html', 'Campanile.html']\n",
      "nonprofit = ['BerkeleyCollege.html']\n",
      "human = ['BerkeleyCollege.html']\n",
      "resources = ['BerkeleyCollege.html']\n",
      "international = ['BerkeleyCollege.html']\n",
      "fashion = ['BerkeleyCollege.html']\n",
      "through = ['BerkeleyCollege.html']\n",
      "larry = ['BerkeleyCollege.html']\n",
      "l = ['BerkeleyCollege.html']\n",
      "luing = ['BerkeleyCollege.html']\n",
      "health = ['BerkeleyCollege.html']\n",
      "criminal = ['BerkeleyCollege.html']\n",
      "legal = ['BerkeleyCollege.html']\n",
      "graphic = ['BerkeleyCollege.html']\n",
      "design = ['BerkeleyCollege.html', 'Campanile.html']\n",
      "interior = ['BerkeleyCollege.html']\n",
      "humanities = ['BerkeleyCollege.html']\n",
      "mathematicsscience = ['BerkeleyCollege.html']\n",
      "social = ['BerkeleyCollege.html']\n",
      "sciences = ['BerkeleyCollege.html']\n",
      "english = ['BerkeleyCollege.html']\n",
      "foreign = ['BerkeleyCollege.html']\n",
      "languages = ['BerkeleyCollege.html']\n",
      "built = ['SouthHall.html']\n",
      "1873 = ['SouthHall.html']\n",
      "building = ['SouthHall.html']\n",
      "heart = ['SouthHall.html']\n",
      "doe = ['SouthHall.html']\n",
      "library = ['SouthHall.html']\n",
      "campanile = ['SouthHall.html', 'Campanile.html']\n",
      "smallest = ['SouthHall.html']\n",
      "bear = ['SouthHall.html']\n",
      "statue = ['SouthHall.html']\n",
      "small = ['SouthHall.html']\n",
      "added = ['SouthHall.html']\n",
      "michael = ['SouthHall.html']\n",
      "h = ['SouthHall.html']\n",
      "casey = ['SouthHall.html']\n",
      "did = ['SouthHall.html']\n",
      "ornamental = ['SouthHall.html']\n",
      "castings = ['SouthHall.html']\n",
      "restored = ['SouthHall.html']\n",
      "facade = ['SouthHall.html']\n",
      "1997 = ['SouthHall.html']\n",
      "aka = ['Campanile.html']\n",
      "bell = ['Campanile.html']\n",
      "clock = ['Campanile.html']\n",
      "commonly = ['Campanile.html']\n",
      "due = ['Campanile.html']\n",
      "resemblance = ['Campanile.html']\n",
      "di = ['Campanile.html']\n",
      "marco = ['Campanile.html']\n",
      "venice = ['Campanile.html']\n",
      "completed = ['Campanile.html']\n",
      "1914 = ['Campanile.html']\n",
      "opened = ['Campanile.html']\n",
      "1917 = ['Campanile.html']\n",
      "stands = ['Campanile.html']\n",
      "307 = ['Campanile.html']\n",
      "feet = ['Campanile.html']\n",
      "936 = ['Campanile.html']\n",
      "m = ['Campanile.html']\n",
      "tall = ['Campanile.html']\n",
      "making = ['Campanile.html']\n",
      "third = ['Campanile.html']\n",
      "tallest = ['Campanile.html']\n",
      "clocktower = ['Campanile.html']\n",
      "world = ['Campanile.html']\n",
      "john = ['Campanile.html']\n",
      "galen = ['Campanile.html']\n",
      "howard = ['Campanile.html']\n",
      "founder = ['Campanile.html']\n",
      "marks = ['Campanile.html']\n",
      "secondary = ['Campanile.html']\n",
      "axis = ['Campanile.html']\n",
      "his = ['Campanile.html']\n",
      "original = ['Campanile.html']\n",
      "beauxarts = ['Campanile.html']\n",
      "plan = ['Campanile.html']\n",
      "since = ['Campanile.html']\n",
      "then = ['Campanile.html']\n",
      "major = ['Campanile.html']\n",
      "point = ['Campanile.html']\n",
      "orientation = ['Campanile.html']\n",
      "almost = ['Campanile.html']\n",
      "every = ['Campanile.html']\n",
      "floors = ['Campanile.html']\n",
      "an = ['Campanile.html']\n",
      "observation = ['Campanile.html']\n",
      "deck = ['Campanile.html']\n",
      "eighth = ['Campanile.html']\n",
      "floor = ['Campanile.html']\n",
      "some = ['Campanile.html']\n",
      "used = ['Campanile.html']\n",
      "store = ['Campanile.html']\n",
      "fossils = ['Campanile.html']\n"
     ]
    }
   ],
   "source": [
    "#part 2\n",
    "\n",
    "\n",
    "\n",
    "#import beautiful soup\n",
    "from bs4 import BeautifulSoup\n",
    "import string\n",
    "\n",
    "#strip word of punctuation and convert to all lower-case\n",
    "def stripWord( w ):\n",
    "    w = w.translate(str.maketrans('', '', string.punctuation))\n",
    "    w = w.lower()\n",
    "    return( w )\n",
    "\n",
    "def webCrawlIndexer():\n",
    "    #start our lexicon\n",
    "    lexicon = {}\n",
    "\n",
    "    # loop over the previously determined webpages\n",
    "    for each in web_pages:\n",
    "        # open them and slurp their data\n",
    "        link = \"https://people.ischool.berkeley.edu/~chuang/i206/b5/\" + each\n",
    "        webUrl  = urllib.request.urlopen(link)\n",
    "        data = str(webUrl.read())\n",
    "\n",
    "        # make them into a soup (yum!)\n",
    "        soup = BeautifulSoup(data, 'html.parser')\n",
    "\n",
    "        # I apologize for my puns it's past my bed time \n",
    "        ingredients = soup.get_text()\n",
    "\n",
    "        # remove special characters\n",
    "        ingredients = ingredients.replace(\"\\\\n\", '')\n",
    "        ingredients = stripWord(ingredients)\n",
    "\n",
    "\n",
    "        # split them into individual words\n",
    "        words = ingredients.split()\n",
    "\n",
    "        # cleanup\n",
    "        #words.remove('californiaxe2x80x94in')\n",
    "        words.remove('b')\n",
    "\n",
    "        # There are a lot of random content when I use get_text, like californiaxe2x80x94in.\n",
    "        # Since the assignment didn't ask me to clean this up, I'm not going to spend too much\n",
    "        # time trying to figure it out since it said to use this command in the first place.\n",
    "        \n",
    "\n",
    "        # TRACING\n",
    "        #print(words)\n",
    "\n",
    "        # loop over the words\n",
    "        for i in range(len(words)):\n",
    "            # strip\n",
    "            words[i] = words[i].strip()\n",
    "            # check if it's already in the dict keys\n",
    "            if words[i] in lexicon.keys():\n",
    "                # if it is, check to see if the page name is already in the value\n",
    "                if each in lexicon[words[i]]:\n",
    "                    pass\n",
    "                # if it isn't, append it\n",
    "                else:\n",
    "                    lexicon[words[i]].append(each)\n",
    "            else:\n",
    "                # otherwise, add the word to the dict\n",
    "                lexicon[words[i]] = [each]\n",
    "\n",
    "    return len(lexicon), lexicon\n",
    "    \n",
    "\n",
    "word_count, lex = webCrawlIndexer()\n",
    "print(\"Our word count is:\", word_count, \"\\n\")\n",
    "print(\"For the record - I did some cleanup, which the assignment did not say I needed to do. It just said to use the beautiful soup module above. If you expected us to remove every single weird entry or duplicate word with some weird characters on the end that get_text seems to find, the assignment description should have said so explicitly.\\n\")\n",
    "for each in lex:\n",
    "    print(each, \"=\", lex[each])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3. Search Query Interface (2 points)\n",
    "\n",
    "Write a search query interface that prompts a user to enter a search query term, and prints a list of web pages corresponding to the query term if it exists in the inverted index from Part 2, or prints \"No results found\" if it does not exist, or quits the interface if the user enters 'q'.\n",
    "\n",
    "For simplicity, the query terms are limited to a single word. You do not need to support search queries with multiple keywords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please enter a word (q to quit): \n",
      "Your word is not in the lexicon. Try again!\n",
      "Please enter a word (q to quit): \n",
      "Your word is not in the lexicon. Try again!\n",
      "Please enter a word (q to quit): \n",
      "Your word is not in the lexicon. Try again!\n",
      "Please enter a word (q to quit): \n",
      "Your word is not in the lexicon. Try again!\n",
      "Please enter a word (q to quit): \n",
      "Your word is not in the lexicon. Try again!\n",
      "Please enter a word (q to quit): \n",
      "Your word is not in the lexicon. Try again!\n",
      "Please enter a word (q to quit): \n",
      "Your word is not in the lexicon. Try again!\n",
      "Please enter a word (q to quit): \n",
      "Your word can be found in the following web pages: ['Berkeley.html', 'ISchool.html', 'CityOfBerkeley.html', 'UCBerkeley.html', 'SouthHall.html']\n",
      "Please enter a word (q to quit): \n",
      "Your word can be found in the following web pages: ['information.html', 'Berkeley.html', 'ISchool.html', 'CityOfBerkeley.html', 'UCBerkeley.html', 'BerkeleyCollege.html']\n",
      "Please enter a word (q to quit): \n",
      "Your word is not in the lexicon. Try again!\n",
      "Please enter a word (q to quit): \n"
     ]
    }
   ],
   "source": [
    "#part 3\n",
    "\n",
    "#define the function\n",
    "def searchInterface():\n",
    "    # take input\n",
    "    print(\"Please enter a word (q to quit): \")\n",
    "    search_in = input()\n",
    "    # keep looping until they hit q\n",
    "    while search_in.lower() != \"q\":\n",
    "        # check if its in the dictionary\n",
    "        if search_in in lex.keys():\n",
    "            print(\"Your word can be found in the following web pages:\", lex[search_in])\n",
    "        # otherwise make them try again\n",
    "        else:\n",
    "            print(\"Your word is not in the lexicon. Try again!\")\n",
    "            \n",
    "        # prompt for input, loopin\n",
    "        print(\"Please enter a word (q to quit): \")\n",
    "        search_in = input()\n",
    "    \n",
    "\n",
    "searchInterface()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra Credit. Search Results Webpage (1 point)\n",
    "\n",
    "Optional: Construct and display a search results webpage (in HTML format) that shows a list of web pages (including actual hyperlinks to the pages) that contain the search term.\n",
    "\n",
    "Python provides an easy way to display a web page with the webbrowser package.  If you run the following, a web browser opens up for you showing the specified page:\n",
    "```\n",
    "import webbrowser\n",
    "webbrowser.open(\"https://ischool.berkeley.edu/\")\n",
    "```\n",
    "\n",
    "If you write your search results webpage out to a local file in your computer, you can use the `webbrowser` command to display it, e.g.,: \n",
    "\n",
    "`webbrowser.open(\"file:///Users/name/Documents/search_results.html\")`\n",
    "\n",
    "The web page should be readable but it does not have to be pretty. Be sure to handle the case where there are no matches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please enter a word (q to quit): \n",
      "Your results have been opened in a browser window.\n",
      "\n",
      "Please enter a word (q to quit): \n",
      "Please enter a word (q to quit): \n",
      "\n",
      "You have chosen to quit. Thanks for playing!\n"
     ]
    }
   ],
   "source": [
    "# part 4\n",
    "\n",
    "# import webbrowser module\n",
    "import webbrowser\n",
    "\n",
    "#define the function\n",
    "def searchInterfacev2():\n",
    "\n",
    "    # prompt for input\n",
    "    print(\"Please enter a word (q to quit): \")\n",
    "    search_in = input()\n",
    "\n",
    "    # keep looping until they enter \"q\"\n",
    "    while search_in.lower() != \"q\":\n",
    "        # start the html contents\n",
    "        html_contents = '''\n",
    "        <html>\n",
    "            <head>\n",
    "                <title> Search Results </title>\n",
    "            </head>\n",
    "            <body>\n",
    "                <h1> 206b Web Crawler Search Results (Assignment 5) </h1>\n",
    "                <p> Your search results are below! </p>\n",
    "        '''\n",
    "        # append some stuff\n",
    "        html_contents += \"<p> Your search term is: <b>\"\n",
    "        html_contents += search_in\n",
    "        html_contents += \"</b></p>\"\n",
    "\n",
    "        # if it's in the lexicon, add some html to include links \n",
    "        if search_in in lex.keys():\n",
    "            print(\"Your results have been opened in a browser window.\\n\")\n",
    "\n",
    "            for each in lex[search_in]:\n",
    "                html_contents += \"<p>\"\n",
    "                html_contents += \"<a href='https://people.ischool.berkeley.edu/~chuang/i206/b5/\"\n",
    "                html_contents += each\n",
    "                html_contents += \"'>\"\n",
    "                html_contents += each\n",
    "                html_contents += \"</a></p>\"\n",
    "\n",
    "        # instructions were unclear if I should handle this in browser window, so I just \n",
    "        # did it to be safe    \n",
    "        else:\n",
    "            html_contents += \"<p> Your word is not in the lexicon. Please switch back to your python window and try again. </p>\"\n",
    "        \n",
    "        # gotta close up those tags!\n",
    "        html_contents += \"</body> </html>\"\n",
    "\n",
    "        # write to file\n",
    "        html_file = open(\"search_output.html\", \"w\")\n",
    "        html_file.write(html_contents)\n",
    "        html_file.close()\n",
    "\n",
    "        # use browser to open the file we just wrote to \n",
    "        webbrowser.open(\"search_output.html\")\n",
    "\n",
    "        # loop again\n",
    "        print(\"Please enter a word (q to quit): \")\n",
    "        search_in = input()\n",
    "\n",
    "    # pretty handling of a quit\n",
    "    print(\"\\nYou have chosen to quit. Thanks for playing!\")\n",
    "    \n",
    "## MAIN\n",
    "searchInterfacev2()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 64-bit (microsoft store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4c98c6d3aee2838e64c8d8685d0345539a7ca4b69b8ca7454491c28cd4729e11"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
