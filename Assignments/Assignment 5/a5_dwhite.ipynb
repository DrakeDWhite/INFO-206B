{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1. Traversing a Web Graph (8 points)\n",
    "\n",
    "You begin by building a web crawler function, webCrawler, that traverses a web graph consisting of a self-contained set of linked web pages.\n",
    "\n",
    "First, you can use the `urllib` package to retrieve web pages as follows:\n",
    "\n",
    "```\n",
    "import urllib.request\n",
    "webUrl  = urllib.request.urlopen('https://ischool.berkeley.edu/')\n",
    "data = webUrl.read()\n",
    "```\n",
    "\n",
    "Starting with the following URL:\n",
    "\n",
    "[https://people.ischool.berkeley.edu/~chuang/i206/b5/index.html](https://people.ischool.berkeley.edu/~chuang/i206/b5/index.html)\n",
    "\n",
    "Your crawler should identify and follow the links on the page, as well as the links found on the other pages reachable from this source page, using the breadth-first search (BFS) technique. \n",
    "\n",
    "You can use regex (which you have now mastered) or the [Beautiful Soup](https://www.crummy.com/software/BeautifulSoup/bs4/doc/). library (e.g., its `findAll()` and `get()` methods) to find the links on each page. To simplify your task, all the links in this set of web pages use relative links, e.g., \n",
    "\n",
    "\\<a href=”somepage.html”>This is a link to some random page\\</a>\n",
    "\n",
    "which should be resolved to:\n",
    "\n",
    "[https://people.ischool.berkeley.edu/~chuang/i206/b5/somepage.html](https://people.ischool.berkeley.edu/~chuang/i206/b5/somepage.html)\n",
    "\n",
    "Note that pages may link to one another via loops, e.g., A links to B, B links to C, and C links back to A. Your crawler has to avoid loops by keeping track of which pages have already been visited (or not), so that you don't visit the same pages again. Use Python's `deque` data structure to implement a queue for this purpose.\n",
    "\n",
    "Note: please do not try to run your code on the open Web unless you have properly implemented the following: (i) checking and conforming to a site’s robots.txt file, (ii) rate-limiting your crawler, (iii) properly resolving fully specified and relative links. Otherwise you may get a nasty call from someone.\n",
    "\n",
    "Upon completion of the crawl, your crawler function should return the following:\n",
    "\n",
    "A list of the pages found (following the exact order in which they were visited, starting with `index.html`)\n",
    "Total number of pages crawled (including `index.html`)\n",
    "Total number of links found\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['information.html']\n"
     ]
    }
   ],
   "source": [
    "#import\n",
    "import urllib.request\n",
    "import re\n",
    "import collections \n",
    "\n",
    "\n",
    "# define webCralwer\n",
    "def webCrawler(link):\n",
    "    #open URL\n",
    "    webUrl  = urllib.request.urlopen(link)\n",
    "    data = str(webUrl.read())\n",
    "    #print(data)\n",
    "\n",
    "    #find links with regex\n",
    "    pattern = re.compile(r'href=[\\'\"]?(.+)[\\'\"]>')\n",
    "    results = pattern.findall(data)\n",
    "    #print(results)\n",
    "\n",
    "webCrawler(\"https://people.ischool.berkeley.edu/~chuang/i206/b5/index.html\")\n",
    "\n",
    "\n",
    "# notes for future drake\n",
    "# https://www.geeksforgeeks.org/deque-in-python/\n",
    "# https://www.youtube.com/watch?v=oDqjPvD54Ss\n",
    "\n",
    "# so we need to us de.count() to check to make sure it isn't already in the queue. \n",
    "# if it isn't then we don't count it.\n",
    "\n",
    "# we also need to count how many pages visited, how many links found, and the list of pages visited\n",
    "# in a specific order\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2. Indexing Web Pages (6 points)\n",
    "\n",
    "Extend your web crawler from Part 1 so that as it encounters web pages, it also builds an inverted index (using the dictionary data structure) based on the words found on each web page. Call this function `webCrawlIndexer`.\n",
    "\n",
    "Each time you retrieve a new web page, you will need to extract the words from the page. You may re-use your code from Assignment 3 (sentiment analysis), or you can also use the `get_text()` method from BeautifulSoup for this purpose.\n",
    "\n",
    "When your indexer encounters a new word, it should add a new entry to the inverted index, with the word as the key, and the page name (e.g., `somepage.html`) as the value. When it encounters a word already in the index, it should update the entry to append the new page name as the value. However, if a word appears multiple times in a web page, you should not append the same web page name multiple times. For example: \n",
    "\n",
    "Correct: inv_index = {‘word1’:[’page1.html,page2.html’]}\n",
    "Incorrect: inv_index = {‘word1’:[’page1.html,page2.html,page2.html’]}\n",
    "\n",
    "Upon completion, your webCrawlIndexer function should return:\n",
    "\n",
    "The number of entries in the inverted index\n",
    "The inverted index dictionary data structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#part 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3. Search Query Interface (2 points)\n",
    "\n",
    "Write a search query interface that prompts a user to enter a search query term, and prints a list of web pages corresponding to the query term if it exists in the inverted index from Part 2, or prints \"No results found\" if it does not exist, or quits the interface if the user enters 'q'.\n",
    "\n",
    "For simplicity, the query terms are limited to a single word. You do not need to support search queries with multiple keywords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#part 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra Credit. Search Results Webpage (1 point)\n",
    "\n",
    "Optional: Construct and display a search results webpage (in HTML format) that shows a list of web pages (including actual hyperlinks to the pages) that contain the search term.\n",
    "\n",
    "Python provides an easy way to display a web page with the webbrowser package.  If you run the following, a web browser opens up for you showing the specified page:\n",
    "```\n",
    "import webbrowser\n",
    "webbrowser.open(\"https://ischool.berkeley.edu/\")\n",
    "```\n",
    "\n",
    "If you write your search results webpage out to a local file in your computer, you can use the `webbrowser` command to display it, e.g.,: \n",
    "\n",
    "`webbrowser.open(\"file:///Users/name/Documents/search_results.html\")`\n",
    "\n",
    "The web page should be readable but it does not have to be pretty. Be sure to handle the case where there are no matches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# part 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8da2674ad6258058d13ee0086267efa2a38eabfca86f71d724f1bc6a8ee69fc3"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
