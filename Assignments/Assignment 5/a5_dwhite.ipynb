{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1. Traversing a Web Graph (8 points)\n",
    "\n",
    "You begin by building a web crawler function, webCrawler, that traverses a web graph consisting of a self-contained set of linked web pages.\n",
    "\n",
    "First, you can use the `urllib` package to retrieve web pages as follows:\n",
    "\n",
    "```\n",
    "import urllib.request\n",
    "webUrl  = urllib.request.urlopen('https://ischool.berkeley.edu/')\n",
    "data = webUrl.read()\n",
    "```\n",
    "\n",
    "Starting with the following URL:\n",
    "\n",
    "[https://people.ischool.berkeley.edu/~chuang/i206/b5/index.html](https://people.ischool.berkeley.edu/~chuang/i206/b5/index.html)\n",
    "\n",
    "Your crawler should identify and follow the links on the page, as well as the links found on the other pages reachable from this source page, using the breadth-first search (BFS) technique. \n",
    "\n",
    "You can use regex (which you have now mastered) or the [Beautiful Soup](https://www.crummy.com/software/BeautifulSoup/bs4/doc/). library (e.g., its `findAll()` and `get()` methods) to find the links on each page. To simplify your task, all the links in this set of web pages use relative links, e.g., \n",
    "\n",
    "\\<a href=”somepage.html”>This is a link to some random page\\</a>\n",
    "\n",
    "which should be resolved to:\n",
    "\n",
    "[https://people.ischool.berkeley.edu/~chuang/i206/b5/somepage.html](https://people.ischool.berkeley.edu/~chuang/i206/b5/somepage.html)\n",
    "\n",
    "Note that pages may link to one another via loops, e.g., A links to B, B links to C, and C links back to A. Your crawler has to avoid loops by keeping track of which pages have already been visited (or not), so that you don't visit the same pages again. Use Python's `deque` data structure to implement a queue for this purpose.\n",
    "\n",
    "Note: please do not try to run your code on the open Web unless you have properly implemented the following: (i) checking and conforming to a site’s robots.txt file, (ii) rate-limiting your crawler, (iii) properly resolving fully specified and relative links. Otherwise you may get a nasty call from someone.\n",
    "\n",
    "Upon completion of the crawl, your crawler function should return the following:\n",
    "\n",
    "A list of the pages found (following the exact order in which they were visited, starting with `index.html`)\n",
    "Total number of pages crawled (including `index.html`)\n",
    "Total number of links found\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Drake's notes #####\n",
    "# make a list of all of them, even with duplicates, and then just iterate over them\n",
    "# don't visit the duplicates? \n",
    "\n",
    "# nvm - this would just go infinitely because we won't know when we run out of content\n",
    "\n",
    "# so we would need to recur for every web page, and stop when we don't find any new URLs \n",
    "\n",
    "# something like while deque != 0?\n",
    "\n",
    "# how do we make it recursive? \n",
    "\n",
    "\n",
    "# get the url\n",
    "# collect all links in the url\n",
    "# visit each of those links\n",
    "    # get all the links in those websites \n",
    "    # if there are links on the website that aren't in the deque, run it on them too \n",
    "    # if link not in dequeu\n",
    "        # append it to the end \n",
    "    \n",
    "# while deque != 0 \n",
    "    # urllib(deque[1])\n",
    "    # find links\n",
    "        # for every link, check if its in the deque \n",
    "        # if it isn't, append it to deque\n",
    "    # if it is\n",
    "        # dequeue.append(link)\n",
    "    # deque.popleft() \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['index.html',\n",
       "  'information.html',\n",
       "  'Berkeley.html',\n",
       "  'ISchool.html',\n",
       "  'MIMS.html',\n",
       "  'CityOfBerkeley.html',\n",
       "  'UCBerkeley.html',\n",
       "  'BerkeleyCollege.html',\n",
       "  'MIMS.html',\n",
       "  'UCBerkeley.html',\n",
       "  'SouthHall.html',\n",
       "  'UCBerkeley.html',\n",
       "  'Campanile.html'],\n",
       " 13,\n",
       " 28)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import\n",
    "import urllib.request\n",
    "import re\n",
    "from collections import deque\n",
    "\n",
    "\n",
    "# define webCralwer\n",
    "def webCrawler(link):\n",
    "\n",
    "    # start queue\n",
    "    queue = deque([])\n",
    "    # add the link to it\n",
    "    queue.append(link)\n",
    "\n",
    "    # keep track of all web pages visited\n",
    "    web_pages = []\n",
    "\n",
    "    # number of links found\n",
    "    num_links = 1\n",
    "\n",
    "    # start the loop\n",
    "    while len(queue) > 0:\n",
    "        \n",
    "        #tracing\n",
    "        #print(queue)\n",
    "\n",
    "        # access the first entry\n",
    "        link = \"https://people.ischool.berkeley.edu/~chuang/i206/b5/\" + queue[0]\n",
    "        webUrl  = urllib.request.urlopen(link)\n",
    "        data = str(webUrl.read())\n",
    "\n",
    "        # for testing with some local files instead\n",
    "        ##file = open(queue[0], \"r\")\n",
    "        ##file_contents = file.read()\n",
    "\n",
    "        # also append to visited pages\n",
    "        web_pages.append(queue[0])\n",
    "        #print(\"All web pages visited:\", web_pages)\n",
    "\n",
    "        # find all the links in the first entry\n",
    "        pattern = re.compile(r'href=[\\'\"]?([^\\'\" >]+)')\n",
    "        results = pattern.findall(data)\n",
    "        ##results = pattern.findall(file_contents)\n",
    "        results = list(results)\n",
    "        # testing\n",
    "        #print(\"Pattern results:\", results)\n",
    "\n",
    "        # loop over all the links in the webpage\n",
    "        for each in results:\n",
    "            # append it to our list of links\n",
    "            num_links += 1\n",
    "            # if it's not in the queue, add it to the end of the queue\n",
    "            if each not in web_pages:\n",
    "                queue.append(each)\n",
    "                # remove the first item \n",
    "        queue.popleft()\n",
    "\n",
    "        #file.close()\n",
    "\n",
    "    return web_pages, len(web_pages), num_links\n",
    "        \n",
    "webCrawler(\"index.html\")\n",
    "\n",
    "\n",
    "# notes for future drake\n",
    "# https://www.geeksforgeeks.org/deque-in-python/\n",
    "# https://www.youtube.com/watch?v=oDqjPvD54Ss\n",
    "\n",
    "# so we need to us de.count() to check to make sure it isn't already in the queue. \n",
    "# if it isn't then we don't count it.\n",
    "\n",
    "# we also need to count how many pages visited, how many links found, and the list of pages visited\n",
    "# in a specific order\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2. Indexing Web Pages (6 points)\n",
    "\n",
    "Extend your web crawler from Part 1 so that as it encounters web pages, it also builds an inverted index (using the dictionary data structure) based on the words found on each web page. Call this function `webCrawlIndexer`.\n",
    "\n",
    "Each time you retrieve a new web page, you will need to extract the words from the page. You may re-use your code from Assignment 3 (sentiment analysis), or you can also use the `get_text()` method from BeautifulSoup for this purpose.\n",
    "\n",
    "When your indexer encounters a new word, it should add a new entry to the inverted index, with the word as the key, and the page name (e.g., `somepage.html`) as the value. When it encounters a word already in the index, it should update the entry to append the new page name as the value. However, if a word appears multiple times in a web page, you should not append the same web page name multiple times. For example: \n",
    "\n",
    "Correct: inv_index = {‘word1’:[’page1.html,page2.html’]}\n",
    "Incorrect: inv_index = {‘word1’:[’page1.html,page2.html,page2.html’]}\n",
    "\n",
    "Upon completion, your webCrawlIndexer function should return:\n",
    "\n",
    "The number of entries in the inverted index\n",
    "The inverted index dictionary data structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#part 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3. Search Query Interface (2 points)\n",
    "\n",
    "Write a search query interface that prompts a user to enter a search query term, and prints a list of web pages corresponding to the query term if it exists in the inverted index from Part 2, or prints \"No results found\" if it does not exist, or quits the interface if the user enters 'q'.\n",
    "\n",
    "For simplicity, the query terms are limited to a single word. You do not need to support search queries with multiple keywords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#part 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra Credit. Search Results Webpage (1 point)\n",
    "\n",
    "Optional: Construct and display a search results webpage (in HTML format) that shows a list of web pages (including actual hyperlinks to the pages) that contain the search term.\n",
    "\n",
    "Python provides an easy way to display a web page with the webbrowser package.  If you run the following, a web browser opens up for you showing the specified page:\n",
    "```\n",
    "import webbrowser\n",
    "webbrowser.open(\"https://ischool.berkeley.edu/\")\n",
    "```\n",
    "\n",
    "If you write your search results webpage out to a local file in your computer, you can use the `webbrowser` command to display it, e.g.,: \n",
    "\n",
    "`webbrowser.open(\"file:///Users/name/Documents/search_results.html\")`\n",
    "\n",
    "The web page should be readable but it does not have to be pretty. Be sure to handle the case where there are no matches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# part 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 64-bit (microsoft store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4c98c6d3aee2838e64c8d8685d0345539a7ca4b69b8ca7454491c28cd4729e11"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
