{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INFO206B Fall 2022 Assignment 3\n",
    "\n",
    "Sentiment analysis uses natural language processing, text analysis, and other methods to systematically identify, extract, quantify, and study affective information in many different contexts. In this assignment, you will perform a simple sentiment analysis on different texts, ranging from tweets to tomes. In the process, you will learn about the design, implementation, and performance evaluation of search algorithms and data structures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1. Tweets (5 points)\n",
    "\n",
    "You are given a [list of famous (or infamous) tweets](https://people.ischool.berkeley.edu/~chuang/i206/b3/tweets.txt). from the colored history of Twitter. Your task is to compute the sentiment of each tweet based on the sentiment scores of the words in the tweet. For this part of the assignment, the sentiment of a tweet is simply the sum of the sentiment scores for each word in the tweet.\n",
    "\n",
    "You will use a hand-coded [sentiment lexicon developed by Finn Årup Nielsen](https://github.com/fnielsen/afinn). that contains a list of ~2,400 English words with sentiment scores ranging from -5 (most negative) to 5 (most positive). Your program will read the file [AFINN-111.txt](https://people.ischool.berkeley.edu/~chuang/i206/b3/AFINN-111.txt) into a dictionary data structure. (Note that the file is tab delimited, so you will want to use the `“\\t”` argument for your split method.)\n",
    "\n",
    "For each of the ten tweets, you should split the tweet into its component words, removing any whitespaces and punctuations, and converting the words to lowercase. Then, you look up the sentiment score for each word in the dictionary. If a word is not found, its score is zero. The sentiment score of the tweet is the sum of the scores of the individual words. Print the tweet and its sentiment score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "#strip word of punctuation and convert to all lower-case\n",
    "def stripWord( w ):\n",
    "    w = w.translate(str.maketrans('', '', string.punctuation))\n",
    "    w = w.lower()\n",
    "    return( w )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweet 0 : just setting up my twttr\n",
      "Tweet sentiment: 0 \n",
      "\n",
      "Tweet 1 : there's a plane in the Hudson. I'm on the ferry going to pick up the people. Crazy.\n",
      "Tweet sentiment: -2 \n",
      "\n",
      "Tweet 2 : Are you ready to celebrate? Well, get ready: We have ICE!!!!! Yes, ICE, *WATER ICE* on Mars! woot!!! Best day ever!!\n",
      "Tweet sentiment: 7 \n",
      "\n",
      "Tweet 3 : Arrested\n",
      "Tweet sentiment: -3 \n",
      "\n",
      "Tweet 4 : HI TWITTERS . THANK YOU FOR A WARM WELCOME. FEELING REALLY 21ST CENTURY .\n",
      "Tweet sentiment: 6 \n",
      "\n",
      "Tweet 5 : Hello Twitterverse! We r now LIVE tweeting from the International Space Station -- the 1st live tweet from Space! :) More soon, send your ?s\n",
      "Tweet sentiment: 0 \n",
      "\n",
      "Tweet 6 : OK, What The Hell Is \"Weird Twitter\"?\n",
      "Tweet sentiment: -6 \n",
      "\n",
      "Tweet 7 : Please retweet this to spread awareness for retweets.\n",
      "Tweet sentiment: 1 \n",
      "\n",
      "Tweet 8 : If only Bradley's arm was longer. Best photo ever. #oscars\n",
      "Tweet sentiment: 3 \n",
      "\n",
      "Tweet 9 : admiring my award winning masterpiece -- super stunning roflcopter tweet ftw woohoo!\n",
      "Tweet sentiment: 31 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# make the lexicon\n",
    "# open the file\n",
    "lexicon_contents = open(\"AFINN-111.txt\", \"r\")\n",
    "# turn it into a list\n",
    "file_lines = lexicon_contents.readlines()\n",
    "\n",
    "# trace trace trace\n",
    "#print(file_lines)\n",
    "\n",
    "# start the dictionary\n",
    "lex_dict = {}\n",
    "\n",
    "# strip the lexicon into list of list \n",
    "for i in range(len(file_lines)):\n",
    "    file_lines[i] = file_lines[i].strip().split(\"\\t\")\n",
    "    # add the keys/values to the dictionary \n",
    "    lex_dict[file_lines[i][0]] = int(file_lines[i][1])\n",
    "\n",
    "#print(file_lines)\n",
    "#print(lex_dict)\n",
    "\n",
    "# close the file\n",
    "lexicon_contents.close()\n",
    "\n",
    "# define the function\n",
    "def tweet_sentiment(in_file):\n",
    "\n",
    "\n",
    "    ### SENTIMENT DICTIONARY IS MADE\n",
    "\n",
    "    # Now it's time for reading the actual file of tweets\n",
    "    tweet_contents = open(in_file, \"r\")\n",
    "    tweet_lines = tweet_contents.readlines()\n",
    "\n",
    "    # loop over contents\n",
    "    for i in range(len(tweet_lines)):\n",
    "        # strip the newline\n",
    "        tweet_lines[i] = tweet_lines[i].strip()\n",
    "        # print out the tweet\n",
    "        print(\"Tweet\", i, \":\", tweet_lines[i])\n",
    "        # now we strip into component words\n",
    "        tweet_lines[i] = tweet_lines[i].split()\n",
    "        \n",
    "        \n",
    "        #define the sentiment\n",
    "        sentiment_max = 0\n",
    "\n",
    "        # now we need to add up the sentiment of all words\n",
    "        for j in range(len(tweet_lines[i])):\n",
    "            # strip word it\n",
    "            the_word = stripWord(tweet_lines[i][j])\n",
    "            \n",
    "            # check to see if it's in there\n",
    "            if the_word in lex_dict:\n",
    "                # if it is, add to the value\n",
    "                sentiment_max += lex_dict[the_word]\n",
    "                #print(the_word, lex_dict[the_word])\n",
    "        #print(tweet_lines[i])\n",
    "        # print it out!\n",
    "        print(\"Tweet sentiment:\", sentiment_max, \"\\n\")\n",
    "    tweet_contents.close()\n",
    "\n",
    "## MAIN \n",
    "tweet_sentiment(\"tweets.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2. Tomes (9 points)\n",
    "\n",
    "Moving from tweets to tomes, we want to evaluate the run-time efficiency of different data structures and search algorithms for supporting sentiment analysis as we scale up the size of the input files.\n",
    "\n",
    "You will measure and compare the run-times of three different search strategies on texts of different sizes.\n",
    "\n",
    "Python's time package provides a timestamp function that you can use:\n",
    "\n",
    "```\n",
    "import time\n",
    "tstart = time.time()\n",
    "\\# the main loop of your code goes here\n",
    "tstop = time.time()\n",
    "elapsed_time = tstop - tstart\n",
    "```\n",
    "\n",
    " \n",
    "\n",
    "Strategy 1 – dictionary lookup: For each word in a tome, look up its sentiment score from the dictionary you constructed in Part 1, and sum up the scores for all the words in the tome. Divide the sum by the number of words to obtain the normalized sentiment score of the tome. Record the elapsed time for processing all the words in your tome. Do not include the time for reading in the text file and constructing the dictionary. Only include your main loop that performs the dictionary lookups for the words.\n",
    "\n",
    "Strategy 2 – linear search: First, construct a new sentiment lexicon using two lists. We will take advantage of the fact that the AFINN file's word entries are already sorted alphabetically. The first list contains the word entries, while the second list contains the word's corresponding sentiment scores. Now, for each word in your tome, perform a linear search for the word in the first list. If and when the word is found, use the list index to look up the word's score in the second list. If you reach the end of the first list and cannot find the word, then the word's score is zero. Once again, sum up the scores for all the words, then compute the normalized sentiment score for the tome. Record the elapsed time for processing all the words in your tome.\n",
    "\n",
    "Strategy 3 – binary search: This strategy is basically the same as Strategy 2, and you should use the same two lists from above. However, for each word in your tome, you perform a binary search instead of a linear search. You can re-use the binary search function that you wrote for Assignment 2. (While your Assignment 2 binary search function was written to search for numbers in a list, it should work for searching for text strings with little or no modification, since the AFINN word entries are already sorted alphabetically.)\n",
    "\n",
    "Write a function for each of the three strategies. For each function, return (i) the number of words in the tome, (ii) the elapsed time, and (ii) the normalized sentiment score of the tome.\n",
    "\n",
    "Run your functions for tomes of different sizes. [Project Gutenberg](https://www.gutenberg.org/) is a good source of long texts:\n",
    "\n",
    "- [The Complete Works of William Shakespeare](https://people.ischool.berkeley.edu/~chuang/i206/b3/shakespeare.txt) (~900k words)\n",
    "\n",
    "- [Les Misérables](https://people.ischool.berkeley.edu/~chuang/i206/b3/les-miserables.txt) (~500k words)\n",
    "- [The Odyssey](https://people.ischool.berkeley.edu/~chuang/i206/b3/odyssey.txt) (~100k words)\n",
    "- [Alice’s Adventure in Wonderland](https://people.ischool.berkeley.edu/~chuang/i206/b3/alice.txt)] (~10k words) \n",
    "\n",
    "In addition to these four tomes, choose a few more of your favorite books.\n",
    "\n",
    "To wrap up your analysis, produce these three outputs:\n",
    "\n",
    "Report the normalized sentiment scores of all the tomes you have analyzed.\n",
    "Use `matplotlib` to generate a graph that plots run-time (in the y-axis) versus tome length in words (x-axis), using Red, Green, and Blue for strategies 1, 2, and 3 respectively.\n",
    "Interpret your results in a few sentences, e.g., how do you interpret the normalized sentiment scores and the graph of run-time vs. tome length for the three strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46\n",
      "Title: alice.txt \n",
      "Word Count: 12763 \n",
      "Elapsed Time: 0.024491071701049805 \n",
      "Normalized Sentiment Score: 0.0036041682989892657 \n",
      "\n",
      "517\n",
      "Title: dracula.txt \n",
      "Word Count: 164382 \n",
      "Elapsed Time: 0.32305312156677246 \n",
      "Normalized Sentiment Score: 0.0031451132119088465 \n",
      "\n",
      "615\n",
      "Title: grimms.txt \n",
      "Word Count: 104168 \n",
      "Elapsed Time: 0.20028090476989746 \n",
      "Normalized Sentiment Score: 0.005903924429767299 \n",
      "\n",
      "-1556\n",
      "Title: les-miserables.txt \n",
      "Word Count: 568531 \n",
      "Elapsed Time: 1.1160945892333984 \n",
      "Normalized Sentiment Score: -0.002736878024241422 \n",
      "\n",
      "101\n",
      "Title: metamorphosis.txt \n",
      "Word Count: 25094 \n",
      "Elapsed Time: 0.049413442611694336 \n",
      "Normalized Sentiment Score: 0.004024866501952658 \n",
      "\n",
      "1974\n",
      "Title: odyssey.txt \n",
      "Word Count: 123600 \n",
      "Elapsed Time: 0.24396586418151855 \n",
      "Normalized Sentiment Score: 0.015970873786407765 \n",
      "\n",
      "12608\n",
      "Title: shakespeare.txt \n",
      "Word Count: 904061 \n",
      "Elapsed Time: 1.7348573207855225 \n",
      "Normalized Sentiment Score: 0.013945961610997488 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "#test_list = [\"death\", \"death\", \"death\", \"death\", \"death\", \"death\", \"death\", \"death\", \"death\", \"death\"]\n",
    "\n",
    "def dict_sent(in_file):\n",
    "    # open the file. Added the encoding argument because I was getting a weird bug\n",
    "    # probably something because I'm bouncing between Mac and PC while I'm doing this \n",
    "    file_contents = open(in_file, \"r\", encoding=\"utf-8\")\n",
    "    file_words = file_contents.read().split()\n",
    "    \n",
    "\n",
    "    # set starting sentiment score\n",
    "    sentiment = 0\n",
    "    # define wordCount since we use it more than once\n",
    "    wordCount = len(file_words)\n",
    "    # debugging, ignore\n",
    "    #bug_list = []\n",
    "    # loop over the file contents\n",
    "    start = time.time()\n",
    "    for i in range(len(file_words)):\n",
    "        # strip the words\n",
    "        file_words[i] = stripWord(file_words[i].strip())\n",
    "        # check if the word is in the dictionary\n",
    "        if file_words[i] in lex_dict:\n",
    "            # add its sentiment if it is found \n",
    "            sentiment += lex_dict[file_words[i]]\n",
    "            #bug_list.append(file_words[i])\n",
    "    stop = time.time()\n",
    "    elapsedTime = stop - start\n",
    "    #bug_list.append(str(len(bug_list)))\n",
    "        \n",
    "    #normalize sentiment\n",
    "    normSent = sentiment / wordCount\n",
    "            \n",
    "    #print(file_words)\n",
    "    \n",
    "    print(sentiment)\n",
    "    #bug_file = open(\"bug_file1.txt\", \"w\")\n",
    "    #bug_list.sort()\n",
    "    #bug_file.write(str(bug_list))\n",
    "    file_contents.close()\n",
    "\n",
    "    return wordCount, elapsedTime, normSent\n",
    "\n",
    "### MAIN\n",
    "# Define our text file list because I'm neurotic with my print formatting\n",
    "text_files = [\"alice.txt\", \"dracula.txt\", \"grimms.txt\", \"les-miserables.txt\", \"metamorphosis.txt\", \"odyssey.txt\", \"shakespeare.txt\"]\n",
    "\n",
    "# Dictionary Search output\n",
    "#print(\"###DICTIONARY SEARCH###\\n\")\n",
    "# for each in text_files:\n",
    "#     wordCount, elapsedTime, normSent = dict_sent(each)\n",
    "#     print(\"Title:\", each, \"\\nWord Count:\", wordCount, \"\\nElapsed Time:\", elapsedTime, \"\\nNormalized Sentiment Score:\", normSent, \"\\n\")\n",
    "\n",
    "#dict_sent(\"test_list.txt\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct the linear lists for the keys and values\n",
    "lex_keys = list(lex_dict.keys())\n",
    "lex_values = list(lex_dict.values())\n",
    "#print(lex_keys)\n",
    "#print(lex_values)\n",
    "\n",
    "# define the linear search function\n",
    "def linear_sent(in_file):\n",
    "    # open the file\n",
    "    file_contents = open(in_file, \"r\", encoding=\"utf-8\")\n",
    "    file_words = file_contents.read().split()\n",
    "    file_contents.close()\n",
    "\n",
    "    # set starting sentiment score\n",
    "    sentiment = 0\n",
    "    # define wordCount since we use it more than once\n",
    "    wordCount = len(file_words)\n",
    "    \n",
    "    # loop over the file contents\n",
    "    start = time.time()\n",
    "    for i in range(len(file_words)):\n",
    "        # strip the words\n",
    "        file_words[i] = stripWord(file_words[i].strip())\n",
    "        # loop over the keys and values \n",
    "        for j in range(len(lex_keys)):\n",
    "            # check each key and see if it equals our value \n",
    "            if lex_keys[j] == file_words[i]:\n",
    "                # if it is, add it to the sentiment and break \n",
    "                sentiment += lex_values[j]\n",
    "                break\n",
    "    stop = time.time()\n",
    "    elapsedTime = stop - start\n",
    "    #print(sentiment)\n",
    "    \n",
    "    # normalize sentiment\n",
    "    normSent = sentiment / wordCount\n",
    "    # return values \n",
    "    return wordCount, elapsedTime, normSent\n",
    "\n",
    "        \n",
    "\n",
    "    #print(file_words)\n",
    "\n",
    "\n",
    "## MAIN\n",
    "#linear_sent(\"dracula.txt\")\n",
    "\n",
    "# Commented out so I don't have to wait an eternity every time I run all cells :)\n",
    "#for each in text_files:\n",
    "#    wordCount, elapsedTime, normSent = linear_sent(each)\n",
    "#    print(\"Title:\", each, \"\\nWord Count:\", wordCount, \"\\nElapsed Time:\", elapsedTime, \"\\nNormalized Sentiment Score:\", normSent, \"\\n\")\n",
    "#print(\"\\n\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the function\n",
    "def binary_search(the_list, key, left_index, right_index):\n",
    "    # doing > because the left_index shouldn't be greater than right index anyway, if they're the same\n",
    "    # there's one element because indexing \n",
    "    if left_index > right_index:\n",
    "        return None\n",
    "   \n",
    "    # printing for testin\n",
    "    # print(the_list)\n",
    "    # calculate the midpoint, round down\n",
    "    midpoint = (left_index + right_index) // 2\n",
    "    # print(\"The midpoint is:\", midpoint)\n",
    "    # printing for checking the math, looks good \n",
    "    #print(midpoint)\n",
    "\n",
    "    # check if our midpoint is the key\n",
    "    if the_list[midpoint] == key:\n",
    "        return midpoint\n",
    "    # if midpoint is greater than our target, then recur on the left half of the list\n",
    "    elif the_list[midpoint] > key:\n",
    "        return binary_search(the_list, key, left_index, midpoint - 1)\n",
    "    # if midpoints is less than our target, then recur on the left half of the list\n",
    "    elif the_list[midpoint] < key:\n",
    "        return binary_search(the_list, key, midpoint + 1, right_index)\n",
    "    # otherwise, it's not there\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: alice.txt \n",
      "Word Count: 12763 \n",
      "Elapsed Time: 0.08806228637695312 \n",
      "Normalized Sentiment Score: 0.0036041682989892657 \n",
      "\n",
      "Title: dracula.txt \n",
      "Word Count: 164382 \n",
      "Elapsed Time: 1.0600242614746094 \n",
      "Normalized Sentiment Score: 0.0031572799941599447 \n",
      "\n",
      "Title: grimms.txt \n",
      "Word Count: 104168 \n",
      "Elapsed Time: 0.6767897605895996 \n",
      "Normalized Sentiment Score: 0.005903924429767299 \n",
      "\n",
      "Title: les-miserables.txt \n",
      "Word Count: 568531 \n",
      "Elapsed Time: 3.588975429534912 \n",
      "Normalized Sentiment Score: -0.0026946639673122484 \n",
      "\n",
      "Title: metamorphosis.txt \n",
      "Word Count: 25094 \n",
      "Elapsed Time: 0.1623997688293457 \n",
      "Normalized Sentiment Score: 0.004024866501952658 \n",
      "\n",
      "Title: odyssey.txt \n",
      "Word Count: 123600 \n",
      "Elapsed Time: 0.7880153656005859 \n",
      "Normalized Sentiment Score: 0.015970873786407765 \n",
      "\n",
      "Title: shakespeare.txt \n",
      "Word Count: 904061 \n",
      "Elapsed Time: 5.761896848678589 \n",
      "Normalized Sentiment Score: 0.013954810571410558 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#define the function\n",
    "def binary_sent(in_file):\n",
    "    # open the file\n",
    "    file_contents = open(in_file, \"r\", encoding=\"utf-8\")\n",
    "    file_words = file_contents.read().split()\n",
    "    file_contents.close()\n",
    "\n",
    "    # set starting sentiment score\n",
    "    sentiment = 0\n",
    "    # define wordCount since we use it more than once\n",
    "    wordCount = len(file_words)\n",
    "    # loop over the file contents\n",
    "    # tracing for debugging, ignore\n",
    "    #bug_list = []\n",
    "    start = time.time()\n",
    "    for i in range(len(file_words)):\n",
    "        # strip the words\n",
    "        file_words[i] = stripWord(file_words[i].strip())\n",
    "        searchIndex = binary_search(lex_keys, file_words[i], 0, len(lex_keys) - 1)\n",
    "        if searchIndex:\n",
    "            sentiment += lex_values[searchIndex]\n",
    "            #bug_list.append(lex_keys[searchIndex])\n",
    "    #bug_file = open(\"bug_file2.txt\", \"w\")\n",
    "    #bug_list.sort()\n",
    "    #bug_list.append(str(len(bug_list)))\n",
    "    #bug_file.write(str(bug_list))\n",
    "    #bug_file\n",
    "    stop = time.time()\n",
    "    elapsedTime = stop - start\n",
    "    normSent = sentiment / wordCount\n",
    "    \n",
    "    return wordCount, elapsedTime, normSent\n",
    "\n",
    "## MAIN\n",
    "#binary_sent(\"test_list.txt\")\n",
    "\n",
    "# for each in text_files:\n",
    "#     wordCount, elapsedTime, normSent = binary_sent(each)\n",
    "#     print(\"Title:\", each, \"\\nWord Count:\", wordCount, \"\\nElapsed Time:\", elapsedTime, \"\\nNormalized Sentiment Score:\", normSent, \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## MAT PLOT LIB code"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6429919fe5eee10fa3db4376c75d0431aac4ee64633f3fde6de3e71a7b7c5c41"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
